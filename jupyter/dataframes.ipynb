{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "2bb98e66-9b05-407d-9e4a-78822124d0f3",
            "metadata": {},
            "source": [
                "# <u>Imports</u>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "a769131b-8441-4a61-aabc-ce1175437427",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pyspark\n",
                "from pyspark.conf import SparkConf\n",
                "from pyspark.sql import Row, Window\n",
                "import pyspark.sql.types as T\n",
                "import pyspark.sql.functions as F"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "558a91bf-e185-41b4-ad43-51e7feeeb574",
            "metadata": {},
            "source": [
                "# <u>Spark Context</u>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "e809645e",
            "metadata": {},
            "outputs": [
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m spark_conf \u001b[38;5;241m=\u001b[39m SparkConf()\n\u001b[0;32m      2\u001b[0m spark_conf\u001b[38;5;241m.\u001b[39msetAll([\n\u001b[0;32m      3\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.master\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark://localhost:7077\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;66;03m# The address of the master node which is set within the docker compose file\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.submit.deployMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;66;03m# Client mode indicates the local host is the driver program (should be client by default)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.executor.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4g\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Explicitly sets the memory allocated to the executor in the cluster (can't exceed amount allocated in the docker compose file)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m ])\n\u001b[1;32m---> 10\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mpyspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspark_conf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\austi\\Source Control\\SparkDeltaLab\\SparkDeltaLab_VirEnv\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
                        "File \u001b[1;32mc:\\Users\\austi\\Source Control\\SparkDeltaLab\\SparkDeltaLab_VirEnv\\Lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
                        "File \u001b[1;32mc:\\Users\\austi\\Source Control\\SparkDeltaLab\\SparkDeltaLab_VirEnv\\Lib\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
                        "File \u001b[1;32mc:\\Users\\austi\\Source Control\\SparkDeltaLab\\SparkDeltaLab_VirEnv\\Lib\\site-packages\\pyspark\\context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
                        "File \u001b[1;32mc:\\Users\\austi\\Source Control\\SparkDeltaLab\\SparkDeltaLab_VirEnv\\Lib\\site-packages\\pyspark\\java_gateway.py:104\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 104\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    110\u001b[0m     )\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "spark_conf = SparkConf()\n",
                "spark_conf.setAll([\n",
                "    (\"spark.master\", \"spark://localhost:7077\"), # The address of the master node which is set within the docker compose file\n",
                "    (\"spark.submit.deployMode\", \"client\"), # Client mode indicates the local host is the driver program (should be client by default)\n",
                "    (\"spark.driver.bindAddress\", \"0.0.0.0\"), # Binds the driver to all available network interfaces\n",
                "    (\"spark.app.name\", \"spark-local-cluster\"), # The name of the application that will display in the Spark UI\n",
                "    (\"spark.executor.memory\", \"4g\") # Explicitly sets the memory allocated to the executor in the cluster (can't exceed amount allocated in the docker compose file)\n",
                "])\n",
                "\n",
                "spark = pyspark.sql.SparkSession.builder.config(conf=spark_conf).getOrCreate()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a7dd504c-cc12-4b3b-95ea-2cea9543c51c",
            "metadata": {},
            "source": [
                "# <u>Creating DataFrames</u>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2f6db460-9e45-4d41-83dd-b17765884065",
            "metadata": {},
            "source": [
                "## Defined Schema"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9e05bde4-f8b6-49a3-804c-418faa5094ba",
            "metadata": {},
            "outputs": [],
            "source": [
                "schema = T.StructType([\n",
                "    T.StructField(\"string_field\", T.StringType(), True),\n",
                "    T.StructField(\"integer_field\", T.IntegerType(), True),\n",
                "    T.StructField(\"float_field\", T.DoubleType(), True),\n",
                "    T.StructField(\"boolean_field\", T.BooleanType(), True),\n",
                "    T.StructField(\"array_field\", T.ArrayType(T.StringType()), True),\n",
                "    T.StructField(\"struct_field\", T.StructType([\n",
                "        T.StructField(\"sub_field\", T.StringType(), True)\n",
                "    ]))\n",
                "])\n",
                "df = spark.createDataFrame(\n",
                "    [\n",
                "        [\"a\", 1, 1.1, True, [\"b\"], {\"sub_field\": \"c\"}],\n",
                "        [\"d\", 2, 2.1, False, [\"e\", \"f\"], {\"sub_field\": \"g\"}],\n",
                "        [\"d\", 3, 3.1, True, [\"h\", \"i\", \"j\"], {\"sub_field\": \"k\"}]\n",
                "    ],\n",
                "    schema\n",
                ")\n",
                "display(df.toPandas())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1fbbd316",
            "metadata": {},
            "outputs": [],
            "source": [
                "schema = T.StructType([\n",
                "    T.StructField(\"name\", T.StringType(), True),\n",
                "    T.StructField(\"age\", T.IntegerType(), True),\n",
                "    T.StructField(\"city\", T.StringType(), True)\n",
                "])\n",
                "data = [\n",
                "    {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"},\n",
                "    {\"name\": \"Bob\", \"age\": 25, \"city\": \"Los Angeles\"},\n",
                "    {\"name\": \"Charlie\", \"age\": 35, \"city\": \"Chicago\"}\n",
                "]\n",
                "df = spark.createDataFrame(data, schema=schema)\n",
                "display(df.toPandas())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c1aa0c58-ab57-4fc9-b8f5-e171378032ec",
            "metadata": {},
            "source": [
                "## Inferred Schema"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4c68cd53-fc55-4c25-8751-c1dfd31b79a4",
            "metadata": {},
            "outputs": [],
            "source": [
                "data = [\n",
                "    Row(name=\"Alice\", age=25, city=\"New York\"),\n",
                "    Row(name=\"Alice\", age=25, city=\"New York\"),\n",
                "    Row(name=\"Bob\", age=30, city=\"San Francisco\"),\n",
                "    Row(name=\"Charlie\", age=35, city=\"Los Angeles\"),\n",
                "    Row(name=\"Charlie\", age=50, city=\"St Louis\")\n",
                "]\n",
                "df = spark.createDataFrame(data)\n",
                "display(df.toPandas())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7a147ec2-812c-424b-bbae-8df31ddeae5e",
            "metadata": {},
            "source": [
                "# <u>Querying DataFrames</u>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "053dd16f-7598-44c5-87ee-c9006a219ef7",
            "metadata": {},
            "source": [
                "## Select"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c5ea636e-0391-4c18-8c8e-79d55b5ed15d",
            "metadata": {},
            "outputs": [],
            "source": [
                "schema = T.StructType([\n",
                "    T.StructField(\"name\", T.StringType(), True),\n",
                "    T.StructField(\"age\", T.IntegerType(), True),\n",
                "    T.StructField(\"city\", T.StringType(), True)\n",
                "])\n",
                "data = [\n",
                "    {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"},\n",
                "    {\"name\": \"Bob\", \"age\": 25, \"city\": \"Los Angeles\"},\n",
                "    {\"name\": \"Charlie\", \"age\": 35, \"city\": \"Chicago\"}\n",
                "]\n",
                "df = spark.createDataFrame(data, schema=schema)\n",
                "display(df.toPandas())\n",
                "\n",
                "selected_df = df.select(\"name\", \"age\")\n",
                "display(selected_df.toPandas())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "62c3b235-4df2-45aa-9337-f3a315847383",
            "metadata": {},
            "source": [
                "## Where"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f2543422-1fae-4a6e-a4c2-ebf5bc7f7625",
            "metadata": {},
            "outputs": [],
            "source": [
                "schema = T.StructType([\n",
                "    T.StructField(\"name\", T.StringType(), True),\n",
                "    T.StructField(\"age\", T.IntegerType(), True),\n",
                "    T.StructField(\"city\", T.StringType(), True),\n",
                "    T.StructField(\"hobbies\", T.ArrayType(T.StringType()), True)\n",
                "])\n",
                "data = [\n",
                "    {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\", \"hobbies\": [\"reading\", \"traveling\"]},\n",
                "    {\"name\": \"Bob\", \"age\": 25, \"city\": \"Los Angeles\", \"hobbies\": [\"sports\", \"music\"]},\n",
                "    {\"name\": \"Charlie\", \"age\": 35, \"city\": \"Chicago\", \"hobbies\": [\"cooking\", \"traveling\"]},\n",
                "    {\"name\": \"David\", \"age\": 22, \"city\": \"Newark\", \"hobbies\": [\"reading\", \"gaming\"]}\n",
                "]\n",
                "df = spark.createDataFrame(data, schema=schema)\n",
                "display(df.toPandas())\n",
                "\n",
                "display(df.where(F.col(\"city\") == \"New York\").toPandas())\n",
                "display(df.where(F.col(\"age\") < 30).toPandas())\n",
                "display(df.where(F.array_contains(F.col(\"hobbies\"), \"traveling\")).toPandas())\n",
                "display(df.where(F.col(\"city\").contains(\"New\")).toPandas())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fc34d4c5-c617-40a8-949c-c51d62da5ed7",
            "metadata": {},
            "source": [
                "## OrderBy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "23f0b223-b86a-42b1-b29a-bbf92f37701e",
            "metadata": {},
            "outputs": [],
            "source": [
                "schema = T.StructType([\n",
                "    T.StructField(\"name\", T.StringType(), True),\n",
                "    T.StructField(\"age\", T.IntegerType(), True),\n",
                "    T.StructField(\"city\", T.StringType(), True)\n",
                "])\n",
                "data = [\n",
                "    {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"},\n",
                "    {\"name\": \"Bob\", \"age\": 25, \"city\": \"Los Angeles\"},\n",
                "    {\"name\": \"Charlie\", \"age\": 35, \"city\": \"Chicago\"}\n",
                "]\n",
                "df = spark.createDataFrame(data, schema=schema)\n",
                "display(df.toPandas())\n",
                "\n",
                "display(df.orderBy(F.col(\"age\").asc()).toPandas())\n",
                "display(df.orderBy(F.col(\"age\").desc()).toPandas())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cf7c2294-2a5e-4325-9fb6-044f76087592",
            "metadata": {},
            "source": [
                "## Distinct"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7747cbae-2084-4bb8-b47e-f719a3156ae1",
            "metadata": {},
            "outputs": [],
            "source": [
                "schema = T.StructType([\n",
                "    T.StructField(\"name\", T.StringType(), True),\n",
                "    T.StructField(\"age\", T.IntegerType(), True),\n",
                "    T.StructField(\"city\", T.StringType(), True)\n",
                "])\n",
                "data = [\n",
                "    {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"},\n",
                "    {\"name\": \"Bob\", \"age\": 25, \"city\": \"Los Angeles\"},\n",
                "    {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"},\n",
                "    {\"name\": \"Charlie\", \"age\": 35, \"city\": \"Chicago\"},\n",
                "    {\"name\": \"Alice\", \"age\": 31, \"city\": \"New York\"}\n",
                "]\n",
                "df = spark.createDataFrame(data, schema=schema)\n",
                "display(df.toPandas())\n",
                "\n",
                "display(df.distinct().toPandas())\n",
                "display(df.select('name').distinct().toPandas())\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d598c924-ecb5-4a43-8853-ccb7a7b3c418",
            "metadata": {},
            "source": [
                "## Count"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f9e18c69-8d1c-4f59-ac72-973633e2aad2",
            "metadata": {},
            "outputs": [],
            "source": [
                "schema = T.StructType([\n",
                "    T.StructField(\"name\", T.StringType(), True),\n",
                "    T.StructField(\"age\", T.IntegerType(), True),\n",
                "    T.StructField(\"city\", T.StringType(), True)\n",
                "])\n",
                "\n",
                "data = [\n",
                "    {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"},\n",
                "    {\"name\": \"Bob\", \"age\": None, \"city\": \"Los Angeles\"},\n",
                "    {\"name\": \"Charlie\", \"age\": 35, \"city\": None},\n",
                "    {\"name\": None, \"age\": 22, \"city\": \"Chicago\"}\n",
                "]\n",
                "df = spark.createDataFrame(data, schema=schema)\n",
                "display(df.toPandas())\n",
                "\n",
                "original_count = df.count()\n",
                "print(f\"Original row count: {original_count}\")\n",
                "\n",
                "cleaned_df = df.dropna()\n",
                "display(cleaned_df.toPandas())\n",
                "cleaned_count = cleaned_df.count()\n",
                "print(f\"Row count after dropping nulls: {cleaned_count}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f22b2940-aa8b-468d-bab6-62582ac57988",
            "metadata": {},
            "source": [
                "## Limit"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b345f8d7-44e8-449d-aa9b-7efb96b1a1b5",
            "metadata": {},
            "outputs": [],
            "source": [
                "schema = T.StructType([\n",
                "    T.StructField(\"name\", T.StringType(), True),\n",
                "    T.StructField(\"age\", T.IntegerType(), True),\n",
                "    T.StructField(\"city\", T.StringType(), True)\n",
                "])\n",
                "data = [\n",
                "    {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"},\n",
                "    {\"name\": \"Bob\", \"age\": 25, \"city\": \"Los Angeles\"},\n",
                "    {\"name\": \"Charlie\", \"age\": 35, \"city\": \"Chicago\"}\n",
                "]\n",
                "df = spark.createDataFrame(data, schema=schema)\n",
                "display(df.toPandas())\n",
                "limited_df = df.limit(2)\n",
                "display(limited_df.toPandas())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a3ca0c70-4766-46bf-ac3b-47ccfef686f4",
            "metadata": {},
            "source": [
                "## Retrieve Value"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "af3ef733-161d-416f-97a6-95f2c5c53375",
            "metadata": {},
            "outputs": [],
            "source": [
                "schema = T.StructType([\n",
                "    T.StructField(\"name\", T.StringType(), True),\n",
                "    T.StructField(\"age\", T.IntegerType(), True),\n",
                "    T.StructField(\"city\", T.StringType(), True)\n",
                "])\n",
                "data = [\n",
                "    {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"},\n",
                "    {\"name\": \"Bob\", \"age\": 25, \"city\": \"Los Angeles\"},\n",
                "    {\"name\": \"Charlie\", \"age\": 35, \"city\": \"Chicago\"}\n",
                "]\n",
                "df = spark.createDataFrame(data, schema=schema)\n",
                "display(df.toPandas())\n",
                "\n",
                "# Retrieve specific value from a row based on equality condition\n",
                "specific_value = df.where(F.col(\"name\") == \"Alice\").select(\"age\").collect()[0][0]\n",
                "print(f\"Age of Alice: {specific_value}\")\n",
                "\n",
                "# Create a Python list from a where condition\n",
                "names_in_ny = df.where(F.col(\"city\") == \"New York\").select(\"name\").rdd.flatMap(lambda x: x).collect()\n",
                "print(f\"Names in New York: {names_in_ny}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d44eccc6-c8b0-4882-8b53-581549e2e8d8",
            "metadata": {},
            "source": [
                "## ToPandas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "59d81bc6-bc59-4bd5-a38d-e8256b440002",
            "metadata": {},
            "outputs": [],
            "source": [
                "result = df_1.toPandas()\n",
                "print(result)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0836aa63-270b-4e5b-a435-abbabc6e898d",
            "metadata": {},
            "source": [
                "## ToJSON"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "01c63b10-4b06-4d66-a740-065de0fdc3d4",
            "metadata": {},
            "outputs": [],
            "source": [
                "result = df_1.toJSON().collect()\n",
                "print(result)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ccc03756-de0a-4edf-84a3-d946bc19987f",
            "metadata": {},
            "source": [
                "# <u>DataFrame Manipulations</u>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c59e43fd-6be4-4373-a198-05e29ca575c7",
            "metadata": {},
            "source": [
                "## WithColumn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "94856b63-d3e8-4630-9468-ddbf34a963ea",
            "metadata": {},
            "outputs": [],
            "source": [
                "ex = df_2.withColumn(\"age_in_5_years\", df_2[\"age\"] + 5)\n",
                "ex.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a6573aaa-698b-460b-bb65-e77abedfd71d",
            "metadata": {},
            "source": [
                "## DropDuplicates"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a545e56f-6cd5-46df-b8a8-51d436cf2f4f",
            "metadata": {},
            "outputs": [],
            "source": [
                "ex = df_2.dropDuplicates()\n",
                "ex.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e522ae1a-e110-4010-abd0-f67edd392f83",
            "metadata": {},
            "outputs": [],
            "source": [
                "ex = df_2.dropDuplicates(['name'])\n",
                "ex.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "71bbea5b-b55f-499e-9dca-904cf146bdc6",
            "metadata": {},
            "source": [
                "## Explode"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5ec8faf5-eaf9-49ba-919b-eec555a161df",
            "metadata": {},
            "outputs": [],
            "source": [
                "ex = df_1.withColumn(\"array_items\", F.explode(df_1[\"array_field\"]))\n",
                "ex.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8619a31c-88f5-4eaa-aa84-095cb97a0222",
            "metadata": {},
            "source": [
                "## GroupBy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "525f9e42-c7fb-417c-abd5-2d77efbddb2d",
            "metadata": {},
            "outputs": [],
            "source": [
                "ex = df_1.groupBy('string_field').agg(\n",
                "    F.sum(\"float_field\").alias(\"total_float\"),\n",
                "    F.avg(\"integer_field\").alias(\"avg_integer\")\n",
                ")\n",
                "ex.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "207ba36f-4aa3-43c0-b6a1-a3400c4ce78b",
            "metadata": {},
            "source": [
                "## Pivot"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d45d4408-0ca1-4068-b9e5-a17acd7f9814",
            "metadata": {},
            "outputs": [],
            "source": [
                "ex = df_1.groupBy(\"string_field\").pivot(\"boolean_field\").agg(F.sum(\"float_field\"))\n",
                "ex.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9db860d9-bccd-4412-bc98-943e124e0c36",
            "metadata": {},
            "source": [
                "## Drop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "04c31520-87c7-4f65-8857-43414226332e",
            "metadata": {},
            "outputs": [],
            "source": [
                "ex = df_2.drop(\"age\")\n",
                "ex.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eaba567f-f434-4740-82bc-576af6442db4",
            "metadata": {},
            "source": [
                "# Monotonically Increasing ID"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ac08cfd5-d355-4c88-a313-ada54f3103f1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Not guranteed to be 1 to N\n",
                "ex = df_2.withColumn(\"ID\", F.monotonically_increasing_id())\n",
                "ex.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f23780ee-d25d-4953-bb3a-592462f0bdb3",
            "metadata": {},
            "source": [
                "# Generating an ID Field"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3397fe6b-291e-47d9-a3a8-a3bec02ad0df",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Guranteed to be 1 to N\n",
                "window_spec = Window.orderBy(F.lit(1))\n",
                "ex = df_2.withColumn(\"ID\", F.row_number().over(window_spec))\n",
                "ex.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4ce33796-9ba6-4618-8914-48da117478e6",
            "metadata": {},
            "source": [
                "# <u>DataFrame Operations</u>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4fa1beb0-0f8e-4d3f-86d2-e22c0436a1cf",
            "metadata": {},
            "source": [
                "# Join"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4efdbbd7-1fe9-4661-af49-4dfbaeef618c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample data\n",
                "employee_data = [(1, \"Alice\", 101), (2, \"Bob\", 102), (3, \"Catherine\", 101), (4, \"Daniel\", 103)]\n",
                "department_data = [(101, \"HR\"), (102, \"IT\"), (103, \"Finance\")]\n",
                "\n",
                "# Create DataFrames\n",
                "df_employee = spark.createDataFrame(employee_data, [\"emp_id\", \"name\", \"dept_id\"])\n",
                "df_department = spark.createDataFrame(department_data, [\"dept_id\", \"dept_name\"])\n",
                "\n",
                "# Perform the join\n",
                "joined_df = df_employee.join(df_department, on=\"dept_id\", how=\"inner\")\n",
                "\n",
                "# Show the result\n",
                "joined_df.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "56eea3c3-0b49-4f62-9f48-03efdce38542",
            "metadata": {},
            "source": [
                "## Union"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "feef1fe8-e7df-49fb-ac97-102597811b8d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample data for January\n",
                "january_data = [(1, \"Alice\", \"January\"), (2, \"Bob\", \"January\")]\n",
                "# Sample data for February\n",
                "february_data = [(3, \"Charlie\", \"February\"), (4, \"David\", \"February\")]\n",
                "\n",
                "# Create DataFrames\n",
                "df_january = spark.createDataFrame(january_data, [\"emp_id\", \"name\", \"month\"])\n",
                "df_february = spark.createDataFrame(february_data, [\"emp_id\", \"name\", \"month\"])\n",
                "\n",
                "# Perform the union\n",
                "union_df = df_january.union(df_february)\n",
                "\n",
                "# Show the result\n",
                "union_df.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f697016f-7174-440c-b200-e53f1ba0383b",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "SparkDeltaLab_VirEnv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
